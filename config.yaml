# config.yaml
training:
  epochs: 10
  lr: 5.0e-6
  batch_size: 16
  attack_steps: 20
  random_steps_per_batch: true

model:
  name: meta-llama/Prompt-Guard-86M
  device: cuda  # auto-detection still occurs, but this overrides

attack:
  strategy: "OneTokenGlobalL2"  # Class name from attacks.py
  params:
    lr_l2: 0.04
    lr_linf: 0.006
    pen_l2: 0.2
    step_size_decay: 0.94
    # Strategy-specific parameters
    radius_overlap: 1.1
    phase2_penalty: 0.05
  attack_steps: 24

datasets:
  train_path: "synthetic-prompt-injections_train.parquet"
  test_path: "synthetic-prompt-injections_test.parquet"
  sample_size:
    train: 10240  # Use null for full dataset
    test: 100